import asyncio
import json
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, AsyncGenerator, List, cast
from enum import Enum
from dataclasses import dataclass, asdict
import logging
import re

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

# æ·»åŠ LangGraphç›¸å…³å¯¼å…¥
from src.graph.async_builder import create_graph
from src.graph.types import State
from src.utils.url_param_generator import generate_url_param
from src.server.repositories.session_repository import (
    SessionRepository,
    get_session_repository,
)

# æ·»åŠ LangChainæ¶ˆæ¯ç±»å‹å¯¼å…¥
from langchain_core.messages import BaseMessage, ToolMessage, AIMessageChunk

logger = logging.getLogger(__name__)


# è‡ªå®šä¹‰JSONç¼–ç å™¨
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, uuid.UUID):
            return str(obj)
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)


def safe_json_dumps(obj):
    """å®‰å…¨çš„JSONåºåˆ—åŒ–"""
    return json.dumps(obj, cls=CustomJSONEncoder, ensure_ascii=False)


# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/api/research", tags=["research"])


# SSEäº‹ä»¶ç±»å‹å®šä¹‰
class SSEEventType(Enum):
    NAVIGATION = "navigation"
    METADATA = "metadata"
    NODE_START = "node_start"
    NODE_COMPLETE = "node_complete"
    PLAN_GENERATED = "plan_generated"
    SEARCH_RESULTS = "search_results"
    AGENT_OUTPUT = "agent_output"
    MESSAGE_CHUNK = "message_chunk"
    ARTIFACT = "artifact"
    PROGRESS = "progress"
    INTERRUPT = "interrupt"  # ğŸ”¥ æ·»åŠ interruptäº‹ä»¶ç±»å‹
    COMPLETE = "complete"
    ERROR = "error"


# è¯·æ±‚æ¨¡å‹
class ActionType(Enum):
    CREATE = "create"
    CONTINUE = "continue"
    FEEDBACK = "feedback"
    MODIFY = "modify"


class ResearchStreamRequest(BaseModel):
    action: ActionType
    message: str
    url_param: Optional[str] = None
    thread_id: Optional[str] = None
    frontend_uuid: str
    frontend_context_uuid: str
    visitor_id: str
    user_id: Optional[str] = None
    config: Dict[str, Any]
    context: Optional[Dict[str, Any]] = None


# SSEäº‹ä»¶æ•°æ®æ¨¡å‹
@dataclass
class NavigationEvent:
    url_param: str
    thread_id: str
    workspace_url: str
    frontend_uuid: str
    frontend_context_uuid: str
    timestamp: str


@dataclass
class MetadataEvent:
    execution_id: str
    thread_id: str
    session_id: Optional[int]  # ğŸ”¥ æ·»åŠ session_idå­—æ®µ
    frontend_uuid: str
    frontend_context_uuid: str
    visitor_id: str
    user_id: Optional[str]
    config_used: Dict[str, Any]
    model_info: Dict[str, str]
    estimated_duration: int
    start_time: str
    execution_type: str = "continue"  # ğŸ”¥ æ·»åŠ execution_typeå­—æ®µï¼šcontinue/feedback/monitor
    timestamp: str


@dataclass
class NodeEvent:
    node_name: str
    node_type: str  # "start" or "complete"
    thread_id: str
    execution_id: str
    input_data: Optional[Dict[str, Any]]
    output_data: Optional[Dict[str, Any]]
    duration_ms: Optional[int]
    timestamp: str


@dataclass
class PlanEvent:
    plan_data: Dict[str, Any]
    plan_iterations: int
    thread_id: str
    execution_id: str
    timestamp: str


@dataclass
class SearchResultsEvent:
    query: str
    results: List[Dict[str, Any]]  # åŒ…å«url, title, content, imagesç­‰
    source: str  # "tavily", "web_search"ç­‰
    thread_id: str
    execution_id: str
    timestamp: str


@dataclass
class AgentOutputEvent:
    agent_name: str
    agent_type: str
    content: str
    metadata: Dict[str, Any]
    thread_id: str
    execution_id: str
    timestamp: str


@dataclass
class MessageChunkEvent:
    chunk_id: str
    content: str
    chunk_type: str  # "planning", "research", "analysis", "conclusion"
    agent_name: str
    sequence: int
    is_final: bool
    metadata: Dict[str, Any]
    thread_id: str
    execution_id: str
    timestamp: str


@dataclass
class ArtifactEvent:
    artifact_id: str
    type: str  # "research_plan", "data_table", "chart", "summary", "code", "document"
    title: str
    content: str
    format: str  # "markdown", "html", "json", "csv", "code"
    metadata: Dict[str, Any]
    thread_id: str
    execution_id: str
    timestamp: str


@dataclass
class ProgressEvent:
    current_node: str
    completed_nodes: List[str]
    remaining_nodes: List[str]
    current_step_description: str
    thread_id: str
    execution_id: str
    timestamp: str


@dataclass
class InterruptEvent:
    interrupt_id: str
    message: str
    options: List[Dict[str, str]]  # [{"text": "å¼€å§‹ç ”ç©¶", "value": "accepted"}]
    thread_id: str
    execution_id: str
    node_name: str  # è§¦å‘interruptçš„èŠ‚ç‚¹åç§°
    timestamp: str


@dataclass
class CompleteEvent:
    execution_id: str
    thread_id: str
    total_duration_ms: int
    tokens_consumed: Dict[str, int]
    total_cost: float
    artifacts_generated: List[str]
    final_status: str
    completion_time: str
    summary: Dict[str, Any]
    timestamp: str


@dataclass
class ErrorEvent:
    error_code: str
    error_message: str
    error_details: Dict[str, Any]
    thread_id: str
    execution_id: str
    retry_after: Optional[int]
    suggestions: List[str]
    timestamp: str


class ResearchStreamService:
    """çœŸå®çš„ç ”ç©¶æµå¼æœåŠ¡"""

    def __init__(self, session_repo: SessionRepository):
        self.session_repo = session_repo
        self._graph = None
        self._token_counter = {"input": 0, "output": 0}
        self._cost_calculator = {"total": 0.0}

    async def _get_graph(self):
        """è·å–æˆ–åˆ›å»ºLangGraphå®ä¾‹"""
        if self._graph is None:
            self._graph = await create_graph()
        return self._graph

    def _calculate_tokens_and_cost(
        self, content: str, is_input: bool = False
    ) -> Dict[str, Any]:
        """è®¡ç®—tokenæ¶ˆè€—å’Œæˆæœ¬"""
        # ç®€å•çš„tokenä¼°ç®—ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨tokenizerï¼‰
        token_count = len(content.split()) * 1.3  # ç²—ç•¥ä¼°ç®—

        if is_input:
            self._token_counter["input"] += token_count
        else:
            self._token_counter["output"] += token_count

        # æˆæœ¬è®¡ç®—ï¼ˆåŸºäºClaude 3.5 Sonnetå®šä»·ï¼‰
        input_cost = (
            self._token_counter["input"] * 0.003 / 1000
        )  # $0.003 per 1K input tokens
        output_cost = (
            self._token_counter["output"] * 0.015 / 1000
        )  # $0.015 per 1K output tokens
        total_cost = input_cost + output_cost

        self._cost_calculator["total"] = total_cost

        return {
            "input_tokens": int(self._token_counter["input"]),
            "output_tokens": int(self._token_counter["output"]),
            "total_cost": round(total_cost, 4),
        }

    def _extract_urls_and_images(self, content: str) -> Dict[str, List[str]]:
        """ä»å†…å®¹ä¸­æå–URLå’Œå›¾ç‰‡é“¾æ¥"""
        url_pattern = r'https?://[^\s<>"{}|\\^`[\]]+[^\s<>"{}|\\^`[\].,;:]'
        img_pattern = (
            r'!\[.*?\]\((https?://[^\s)]+)\)|<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        )

        urls = re.findall(url_pattern, content)
        img_matches = re.findall(img_pattern, content)
        images = [match[0] or match[1] for match in img_matches if match[0] or match[1]]

        return {"urls": list(set(urls)), "images": list(set(images))}

    def _get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return datetime.utcnow().isoformat() + "Z"

    def _determine_chunk_type(self, node_name: str, content: str) -> str:
        """æ ¹æ®èŠ‚ç‚¹åç§°å’Œå†…å®¹ç¡®å®šå—ç±»å‹"""
        node_type_map = {
            "coordinator": "planning",
            "planner": "planning",
            "background_investigator": "research",
            "researcher": "research",
            "coder": "analysis",
            "reporter": "conclusion",
        }
        return node_type_map.get(node_name, "research")

    def _parse_search_results(self, search_content: str) -> List[Dict[str, Any]]:
        """è§£ææœç´¢ç»“æœå†…å®¹"""
        results = []
        # ç®€å•çš„è§£æé€»è¾‘ï¼Œå®é™…åº”è¯¥æ›´å¤æ‚
        sections = search_content.split("## ")
        for section in sections[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºæ®µ
            lines = section.strip().split("\n", 1)
            if len(lines) >= 2:
                title = lines[0].strip()
                content = lines[1].strip()

                # æå–URLå’Œå›¾ç‰‡
                extracted = self._extract_urls_and_images(content)

                results.append(
                    {
                        "title": title,
                        "content": content[:500],  # é™åˆ¶é•¿åº¦
                        "url": extracted["urls"][0] if extracted["urls"] else "",
                        "images": extracted["images"],
                        "snippet": content[:200],
                    }
                )

        return results

    def _get_remaining_nodes(
        self, current_node: str, completed_nodes: List[str]
    ) -> List[str]:
        """è·å–å‰©ä½™èŠ‚ç‚¹åˆ—è¡¨"""
        all_nodes = [
            "coordinator",
            "background_investigator",
            "planner",
            "researcher",
            "reporter",
        ]
        remaining = []

        current_index = (
            all_nodes.index(current_node) if current_node in all_nodes else 0
        )
        for node in all_nodes[current_index + 1 :]:
            if node not in completed_nodes:
                remaining.append(node)

        return remaining

    def _get_node_description(self, node_name: str) -> str:
        """è·å–èŠ‚ç‚¹æè¿°"""
        descriptions = {
            "coordinator": "æ­£åœ¨åè°ƒç ”ç©¶ä»»åŠ¡...",
            "background_investigator": "æ­£åœ¨è¿›è¡ŒèƒŒæ™¯è°ƒç ”...",
            "planner": "æ­£åœ¨åˆ¶å®šç ”ç©¶è®¡åˆ’...",
            "researcher": "æ­£åœ¨æ‰§è¡Œæ·±åº¦ç ”ç©¶...",
            "coder": "æ­£åœ¨å¤„ç†æ•°æ®åˆ†æ...",
            "reporter": "æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...",
        }
        return descriptions.get(node_name, f"æ­£åœ¨æ‰§è¡Œ{node_name}...")

    async def _process_langgraph_stream(
        self,
        graph,
        initial_state: Dict[str, Any],
        thread_id: str,
        execution_id: str,
        request: ResearchStreamRequest,
        execution_type: str = "continue",
    ) -> AsyncGenerator[Dict[str, str], None]:
        """å¤„ç†LangGraphæµå¼æ‰§è¡Œ"""

        # è·å–sessionä¿¡æ¯ç”¨äºæ•°æ®åº“ä¿å­˜
        session = await self.session_repo.get_session_by_thread_id(thread_id)
        if not session:
            raise ValueError(f"Session not found for thread_id: {thread_id}")

        start_time = datetime.utcnow()
        current_node = ""
        completed_nodes = []

        try:
            # å‘é€å¼€å§‹äº‹ä»¶
            metadata_event = MetadataEvent(
                execution_id=execution_id,
                thread_id=thread_id,
                session_id=session.id,
                frontend_uuid=request.frontend_uuid,
                frontend_context_uuid=request.frontend_context_uuid,
                visitor_id=request.visitor_id,
                user_id=request.user_id,
                config_used=request.config,
                model_info={
                    "model_name": (
                        request.config.get("model_config", {}).get(
                            "model_name", "claude-3-5-sonnet-20241022"
                        )
                    ),
                    "provider": (
                        request.config.get("model_config", {}).get(
                            "provider", "anthropic"
                        )
                    ),
                    "version": "20241022",
                },
                estimated_duration=120,
                start_time=start_time.isoformat() + "Z",
                execution_type=execution_type,
                timestamp=self._get_current_timestamp(),
            )

            yield {
                "event": SSEEventType.METADATA.value,
                "data": safe_json_dumps(asdict(metadata_event)),
            }

            # æ‰§è¡ŒLangGraphå·¥ä½œæµ
            config = {"configurable": {"thread_id": thread_id}}

            # ğŸ”¥ ä½¿ç”¨messageså’Œupdatesæ··åˆæ¨¡å¼æ¥æ•è·interruptäº‹ä»¶ï¼ˆå‚è€ƒapp.pyå®ç°ï¼‰
            async for agent, _, event_data in graph.astream(
                initial_state,
                config,
                stream_mode=["messages", "updates"],
                subgraphs=True,
            ):
                current_timestamp = self._get_current_timestamp()

                # ğŸ”¥ å¤„ç†LangGraphäº‹ä»¶ - å®Œå…¨å‚è€ƒapp.pyçš„æ­£ç¡®å®ç°
                if isinstance(event_data, dict):
                    if "__interrupt__" in event_data:
                        # å¤„ç†interruptäº‹ä»¶ï¼ˆå®Œå…¨å‚è€ƒapp.pyå®ç°ï¼‰
                        interrupt_data = event_data["__interrupt__"][0]
                        interrupt_value = interrupt_data.value

                        logger.info(f"ğŸ”„ æ”¶åˆ°interruptäº‹ä»¶: {interrupt_value}")

                        # æ£€æŸ¥æ˜¯å¦æ˜¯reaskç±»å‹çš„interrupt
                        if (
                            isinstance(interrupt_value, tuple)
                            and len(interrupt_value) == 2
                            and interrupt_value[0] == "reask"
                        ):
                            # å¤„ç†reask interrupt
                            original_input = interrupt_value[1]
                            reask_event = {
                                "thread_id": thread_id,
                                "id": interrupt_data.ns[0],
                                "role": "assistant",
                                "content": "æ­£åœ¨æ¢å¤åŸå§‹è¾“å…¥çŠ¶æ€...",
                                "finish_reason": "reask",
                                "original_input": original_input,
                            }
                            yield {
                                "event": "reask",
                                "data": safe_json_dumps(reask_event),
                            }
                        else:
                            # å¤„ç†æ ‡å‡†interrupt
                            # æ£€æŸ¥ interrupt_value æ˜¯å¦åŒ…å« options
                            if (
                                isinstance(interrupt_value, dict)
                                and "options" in interrupt_value
                            ):
                                message_content = interrupt_value.get(
                                    "message", "Please Review the Plan."
                                )
                                options = interrupt_value.get("options", [])
                            else:
                                # å…¼å®¹æ—§æ ¼å¼
                                message_content = str(interrupt_value)
                                options = [
                                    {"text": "å¼€å§‹ç ”ç©¶", "value": "accepted"},
                                    {"text": "ç¼–è¾‘è®¡åˆ’", "value": "edit_plan"},
                                    {"text": "ç«‹å³ç”ŸæˆæŠ¥å‘Š", "value": "skip_research"},
                                    {"text": "é‡æ–°æé—®", "value": "reask"},
                                ]

                            interrupt_event = {
                                "thread_id": thread_id,
                                "id": interrupt_data.ns[0],
                                "role": "assistant",
                                "content": message_content,
                                "finish_reason": "interrupt",
                                "options": options,
                            }

                            yield {
                                "event": SSEEventType.INTERRUPT.value,
                                "data": safe_json_dumps(interrupt_event),
                            }

                        # interruptåä¸å‘é€completeäº‹ä»¶ï¼Œç­‰å¾…ç”¨æˆ·åé¦ˆ
                        logger.info(f"ğŸ”„ Interruptå‘é€å®Œæˆï¼Œç­‰å¾…ç”¨æˆ·åé¦ˆ")
                        return
                    else:
                        # å¤„ç†updatesäº‹ä»¶
                        for node_name, node_output in event_data.items():
                            if node_name == "__start__":
                                continue

                            # å‘é€èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                            if node_name != current_node:
                                # å¦‚æœä¹‹å‰æœ‰èŠ‚ç‚¹åœ¨æ‰§è¡Œï¼Œå‘é€å®Œæˆäº‹ä»¶
                                if current_node and current_node != "coordinator":
                                    completed_nodes.append(current_node)
                                    node_complete_event = NodeEvent(
                                        node_name=current_node,
                                        node_type="complete",
                                        thread_id=thread_id,
                                        execution_id=execution_id,
                                        input_data=None,
                                        output_data=None,
                                        duration_ms=None,
                                        timestamp=current_timestamp,
                                    )

                                    yield {
                                        "event": SSEEventType.NODE_COMPLETE.value,
                                        "data": safe_json_dumps(
                                            asdict(node_complete_event)
                                        ),
                                    }

                                # å‘é€æ–°èŠ‚ç‚¹å¼€å§‹äº‹ä»¶
                                current_node = node_name
                                node_start_event = NodeEvent(
                                    node_name=current_node,
                                    node_type="start",
                                    thread_id=thread_id,
                                    execution_id=execution_id,
                                    input_data=None,
                                    output_data=None,
                                    duration_ms=None,
                                    timestamp=current_timestamp,
                                )

                                yield {
                                    "event": SSEEventType.NODE_START.value,
                                    "data": safe_json_dumps(asdict(node_start_event)),
                                }

                            # å¤„ç†èŠ‚ç‚¹è¾“å‡º
                            if isinstance(node_output, dict):
                                # å¤„ç†æ¶ˆæ¯
                                if "messages" in node_output:
                                    messages = node_output["messages"]
                                    if isinstance(messages, list) and messages:
                                        last_message = messages[-1]
                                        content = getattr(
                                            last_message, "content", str(last_message)
                                        )

                                        if content:
                                            # ğŸ”¥ ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“
                                            try:
                                                await self.session_repo.save_message(
                                                    session_id=session.id,
                                                    execution_id=execution_id,
                                                    role="assistant",
                                                    content=content,
                                                    content_type="text",
                                                    frontend_context_uuid=request.frontend_context_uuid,
                                                    source_agent=node_name,
                                                )
                                            except Exception as e:
                                                logger.error(f"ä¿å­˜æ¶ˆæ¯å¤±è´¥: {e}")

                                            # è®¡ç®—tokenå’Œæˆæœ¬
                                            token_info = (
                                                self._calculate_tokens_and_cost(
                                                    content, is_input=False
                                                )
                                            )

                                            # æå–URLå’Œå›¾ç‰‡
                                            extracted = self._extract_urls_and_images(
                                                content
                                            )

                                            # å‘é€æ¶ˆæ¯å—äº‹ä»¶
                                            chunk_event = MessageChunkEvent(
                                                chunk_id=str(uuid.uuid4()),
                                                content=content,
                                                chunk_type=self._determine_chunk_type(
                                                    current_node, content
                                                ),
                                                agent_name=node_name,
                                                sequence=len(messages),
                                                is_final=False,
                                                metadata={
                                                    "urls": extracted["urls"],
                                                    "images": extracted["images"],
                                                    "token_info": token_info,
                                                    "node": current_node,
                                                },
                                                thread_id=thread_id,
                                                execution_id=execution_id,
                                                timestamp=current_timestamp,
                                            )

                                            yield {
                                                "event": (
                                                    SSEEventType.MESSAGE_CHUNK.value
                                                ),
                                                "data": safe_json_dumps(
                                                    asdict(chunk_event)
                                                ),
                                            }

                                # å¤„ç†è®¡åˆ’ç”Ÿæˆ
                                if "current_plan" in node_output:
                                    plan_data = node_output["current_plan"]

                                    # ğŸ”¥ ä¿å­˜planä½œä¸ºartifactåˆ°æ•°æ®åº“
                                    try:
                                        # å¤„ç†Planå¯¹è±¡çš„åºåˆ—åŒ–
                                        if isinstance(plan_data, str):
                                            plan_content = plan_data
                                        elif hasattr(plan_data, "dict"):
                                            # Pydanticæ¨¡å‹ï¼Œä½¿ç”¨.dict()æ–¹æ³•
                                            plan_content = json.dumps(
                                                plan_data.dict(),
                                                ensure_ascii=False,
                                                indent=2,
                                            )
                                        elif hasattr(plan_data, "__dict__"):
                                            # æ™®é€šå¯¹è±¡ï¼Œä½¿ç”¨__dict__
                                            plan_content = json.dumps(
                                                plan_data.__dict__,
                                                ensure_ascii=False,
                                                indent=2,
                                            )
                                        else:
                                            # å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨safe_json_dumps
                                            plan_content = safe_json_dumps(plan_data)

                                        await self.session_repo.save_artifact(
                                            session_id=session.id,
                                            execution_id=execution_id,
                                            artifact_type="research_plan",
                                            title="ç ”ç©¶è®¡åˆ’",
                                            content=plan_content,
                                            description="ç”±AIç”Ÿæˆçš„ç ”ç©¶è®¡åˆ’",
                                            content_format="json",
                                            source_agent=node_name,
                                            generation_context={
                                                "node": current_node,
                                                "timestamp": current_timestamp,
                                            },
                                        )
                                    except Exception as e:
                                        logger.error(f"ä¿å­˜plan artifactå¤±è´¥: {e}")
                                        logger.error(
                                            f"Plan data type: {type(plan_data)}"
                                        )
                                        logger.error(
                                            f"Plan data content: {str(plan_data)[:200]}..."
                                        )

                                    plan_event = PlanEvent(
                                        plan_data=(
                                            plan_data
                                            if isinstance(plan_data, dict)
                                            else {"plan": str(plan_data)}
                                        ),
                                        plan_iterations=node_output.get(
                                            "plan_iterations", 0
                                        ),
                                        thread_id=thread_id,
                                        execution_id=execution_id,
                                        timestamp=current_timestamp,
                                    )

                                    yield {
                                        "event": SSEEventType.PLAN_GENERATED.value,
                                        "data": safe_json_dumps(asdict(plan_event)),
                                    }

                                # å¤„ç†èƒŒæ™¯è°ƒæŸ¥ç»“æœ
                                if "background_investigation_results" in node_output:
                                    search_content = node_output[
                                        "background_investigation_results"
                                    ]

                                    # ğŸ”¥ ä¿å­˜æœç´¢ç»“æœåˆ°æ•°æ®åº“
                                    try:
                                        await self.session_repo.save_message(
                                            session_id=session.id,
                                            execution_id=execution_id,
                                            role="assistant",
                                            content=search_content,
                                            content_type="search_results",
                                            frontend_context_uuid=request.frontend_context_uuid,
                                            source_agent=node_name,
                                        )
                                    except Exception as e:
                                        logger.error(f"ä¿å­˜æœç´¢ç»“æœå¤±è´¥: {e}")

                                    extracted = self._extract_urls_and_images(
                                        search_content
                                    )

                                    # è§£ææœç´¢ç»“æœ
                                    search_results = self._parse_search_results(
                                        search_content
                                    )

                                    search_event = SearchResultsEvent(
                                        query=node_output.get("research_topic", ""),
                                        results=search_results,
                                        source="tavily",
                                        thread_id=thread_id,
                                        execution_id=execution_id,
                                        timestamp=current_timestamp,
                                    )

                                    yield {
                                        "event": SSEEventType.SEARCH_RESULTS.value,
                                        "data": safe_json_dumps(asdict(search_event)),
                                    }

                                # å¤„ç†æœ€ç»ˆæŠ¥å‘Š
                                if "final_report" in node_output:
                                    report_content = node_output["final_report"]

                                    # ğŸ”¥ ä¿å­˜æœ€ç»ˆæŠ¥å‘Šåˆ°æ•°æ®åº“
                                    try:
                                        await self.session_repo.save_artifact(
                                            session_id=session.id,
                                            execution_id=execution_id,
                                            artifact_type="summary",
                                            title="ç ”ç©¶æŠ¥å‘Š",
                                            content=report_content,
                                            description="æœ€ç»ˆç ”ç©¶æŠ¥å‘Š",
                                            content_format="markdown",
                                            source_agent=node_name,
                                            generation_context={
                                                "node": current_node,
                                                "research_topic": node_output.get(
                                                    "research_topic", ""
                                                ),
                                            },
                                        )
                                    except Exception as e:
                                        logger.error(f"ä¿å­˜æœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")

                                    # åˆ›å»ºæŠ¥å‘Šartifact
                                    artifact_event = ArtifactEvent(
                                        artifact_id=str(uuid.uuid4()),
                                        type="summary",
                                        title="ç ”ç©¶æŠ¥å‘Š",
                                        content=report_content,
                                        format="markdown",
                                        metadata={
                                            "node": current_node,
                                            "research_topic": node_output.get(
                                                "research_topic", ""
                                            ),
                                        },
                                        thread_id=thread_id,
                                        execution_id=execution_id,
                                        timestamp=current_timestamp,
                                    )

                                    yield {
                                        "event": SSEEventType.ARTIFACT.value,
                                        "data": safe_json_dumps(asdict(artifact_event)),
                                    }

                            # å‘é€è¿›åº¦æ›´æ–°
                            remaining_nodes = self._get_remaining_nodes(
                                current_node, completed_nodes
                            )
                            progress_event = ProgressEvent(
                                current_node=current_node,
                                completed_nodes=completed_nodes.copy(),
                                remaining_nodes=remaining_nodes,
                                current_step_description=self._get_node_description(
                                    current_node
                                ),
                                thread_id=thread_id,
                                execution_id=execution_id,
                                timestamp=current_timestamp,
                            )

                            yield {
                                "event": SSEEventType.PROGRESS.value,
                                "data": safe_json_dumps(asdict(progress_event)),
                            }

                # ğŸ”¥ å¤„ç†messagesæµ - å®Œå…¨å‚è€ƒapp.pyå®ç°
                try:
                    # å®‰å…¨åœ°å¤„ç†å¯èƒ½çš„å•å…ƒç´ æˆ–åŒå…ƒç´ æƒ…å†µ
                    if isinstance(event_data, tuple) and len(event_data) >= 2:
                        message_chunk, message_metadata = cast(
                            tuple[BaseMessage, dict[str, any]], event_data
                        )
                    elif isinstance(event_data, tuple) and len(event_data) == 1:
                        message_chunk = event_data[0]
                        message_metadata = {}
                    else:
                        # å¦‚æœä¸æ˜¯é¢„æœŸçš„æ ¼å¼ï¼Œè·³è¿‡å¤„ç†
                        continue

                    # ç¡®ä¿message_chunkæ˜¯BaseMessageç±»å‹
                    if not isinstance(message_chunk, BaseMessage):
                        continue

                    event_stream_message: dict[str, any] = {
                        "thread_id": thread_id,
                        "agent": agent[0].split(":")[0] if agent else "unknown",
                        "id": message_chunk.id,
                        "role": "assistant",
                        "content": message_chunk.content,
                    }

                    if message_chunk.additional_kwargs.get("reasoning_content"):
                        event_stream_message["reasoning_content"] = (
                            message_chunk.additional_kwargs["reasoning_content"]
                        )
                    if message_chunk.response_metadata.get("finish_reason"):
                        event_stream_message["finish_reason"] = (
                            message_chunk.response_metadata.get("finish_reason")
                        )

                    if isinstance(message_chunk, ToolMessage):
                        # Tool Message - Return the result of the tool call
                        event_stream_message["tool_call_id"] = (
                            message_chunk.tool_call_id
                        )
                        yield {
                            "event": "tool_call_result",
                            "data": safe_json_dumps(event_stream_message),
                        }
                    elif isinstance(message_chunk, AIMessageChunk):
                        # AI Message - Raw message tokens
                        if message_chunk.tool_calls:
                            # AI Message - Tool Call
                            event_stream_message["tool_calls"] = (
                                message_chunk.tool_calls
                            )
                            event_stream_message["tool_call_chunks"] = (
                                message_chunk.tool_call_chunks
                            )
                            yield {
                                "event": "tool_calls",
                                "data": safe_json_dumps(event_stream_message),
                            }
                        elif message_chunk.tool_call_chunks:
                            # AI Message - Tool Call Chunks
                            event_stream_message["tool_call_chunks"] = (
                                message_chunk.tool_call_chunks
                            )
                            yield {
                                "event": "tool_call_chunks",
                                "data": safe_json_dumps(event_stream_message),
                            }
                        else:
                            # AI Message - Raw message tokens
                            yield {
                                "event": SSEEventType.MESSAGE_CHUNK.value,
                                "data": safe_json_dumps(event_stream_message),
                            }
                except Exception as message_error:
                    logger.warning(
                        f"âš ï¸ Messagesæµå¤„ç†é”™è¯¯: {message_error}, event_dataç±»å‹: {type(event_data)}"
                    )
                    # ä¸ä¸­æ–­æµï¼Œç»§ç»­å¤„ç†å…¶ä»–äº‹ä»¶
                    continue

            # ğŸ”¥ æ£€æŸ¥LangGraphæ˜¯å¦çœŸæ­£å®Œæˆ
            try:
                config = {"configurable": {"thread_id": thread_id}}
                current_state = await graph.aget_state(config)

                # ğŸ”¥ ä¿®å¤interruptçŠ¶æ€æ£€æµ‹é€»è¾‘
                has_interrupt = False
                if (
                    current_state
                    and hasattr(current_state, "tasks")
                    and current_state.tasks
                ):
                    # æ£€æŸ¥æ˜¯å¦æœ‰pendingçš„interruptä»»åŠ¡
                    for task in current_state.tasks:
                        if hasattr(task, "interrupts") and task.interrupts:
                            has_interrupt = True
                            break

                # æ›´å‡†ç¡®çš„å®ŒæˆçŠ¶æ€åˆ¤æ–­
                is_truly_complete = (
                    current_state
                    and not has_interrupt  # æ²¡æœ‰pendingçš„interrupt
                    and (
                        not current_state.next or len(current_state.next) == 0
                    )  # æ²¡æœ‰ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                )

                logger.info(
                    f"ğŸ” LangGraphçŠ¶æ€æ£€æŸ¥ - æ˜¯å¦æœ‰interrupt: {has_interrupt}, æ˜¯å¦çœŸæ­£å®Œæˆ: {is_truly_complete}, nextèŠ‚ç‚¹: {current_state.next if current_state else 'None'}"
                )

                if is_truly_complete:
                    # åªæœ‰åœ¨çœŸæ­£å®Œæˆæ—¶æ‰å‘é€completeäº‹ä»¶
                    end_time = datetime.utcnow()
                    duration_ms = int((end_time - start_time).total_seconds() * 1000)
                    final_token_info = self._calculate_tokens_and_cost(
                        "", is_input=False
                    )

                    complete_event = CompleteEvent(
                        execution_id=execution_id,
                        thread_id=thread_id,
                        total_duration_ms=duration_ms,
                        tokens_consumed=final_token_info,
                        total_cost=0.0,
                        artifacts_generated=[],
                        final_status="completed",
                        completion_time=self._get_current_timestamp(),
                        summary={"nodes_completed": completed_nodes},
                        timestamp=self._get_current_timestamp(),
                    )

                    yield {
                        "event": SSEEventType.COMPLETE.value,
                        "data": safe_json_dumps(asdict(complete_event)),
                    }
                else:
                    # å·¥ä½œæµæœªå®Œæˆï¼ˆå¯èƒ½åœ¨interruptç­‰å¾…ç”¨æˆ·åé¦ˆï¼‰
                    logger.info(
                        f"ğŸ”„ LangGraphå·¥ä½œæµæœªå®Œæˆï¼Œç­‰å¾…åç»­æ“ä½œ - interrupt: {has_interrupt}"
                    )

            except Exception as state_error:
                logger.warning(f"âš ï¸ æ— æ³•è·å–LangGraphçŠ¶æ€: {state_error}")
                # å¦‚æœæ— æ³•è·å–çŠ¶æ€ï¼Œä¿å®ˆåœ°ä¸å‘é€completeäº‹ä»¶

        except Exception as e:
            logger.error(f"LangGraphæ‰§è¡Œé”™è¯¯: {e}")
            error_event = ErrorEvent(
                error_code="LANGGRAPH_EXECUTION_ERROR",
                error_message=str(e),
                error_details={"traceback": str(e)},
                thread_id=thread_id,
                execution_id=execution_id,
                retry_after=None,
                suggestions=["æ£€æŸ¥é…ç½®", "é‡è¯•è¯·æ±‚"],
                timestamp=self._get_current_timestamp(),
            )

            yield {
                "event": SSEEventType.ERROR.value,
                "data": safe_json_dumps(asdict(error_event)),
            }

    async def create_research_stream(
        self,
        request: ResearchStreamRequest,
        existing_session_id: Optional[int] = None,
        existing_thread_id: Optional[str] = None,
    ) -> AsyncGenerator[Dict[str, str], None]:
        """åˆ›å»ºæ–°çš„ç ”ç©¶æµ"""
        try:
            # è§£æé…ç½® - æ”¯æŒæ–°æ—§æ ¼å¼
            # å¦‚æœæœ‰research_configå­—æ®µï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä»æ‰å¹³åŒ–çš„configä¸­æå–
            if "research_config" in request.config:
                research_config = request.config["research_config"]
            else:
                # ä»æ‰å¹³åŒ–çš„configä¸­æå–researchç›¸å…³é…ç½®
                research_config = {
                    "auto_accepted_plan": request.config.get(
                        "auto_accepted_plan", False
                    ),
                    "enable_background_investigation": request.config.get(
                        "enableBackgroundInvestigation", True
                    ),
                    "report_style": request.config.get("reportStyle", "academic"),
                    "enable_deep_thinking": request.config.get(
                        "enableDeepThinking", False
                    ),
                    "max_plan_iterations": request.config.get("maxPlanIterations", 3),
                    "max_step_num": request.config.get("maxStepNum", 5),
                    "max_search_results": request.config.get("maxSearchResults", 5),
                }

            model_config = request.config.get("model_config", {})
            output_config = request.config.get(
                "output_config",
                {
                    "language": "zh-CN",
                    "output_format": request.config.get("outputFormat", "markdown"),
                },
            )

            # ğŸ”¥ æ ¹æ®æ˜¯å¦æœ‰ç°æœ‰sessionå†³å®šåˆ›å»ºæˆ–å¤ç”¨
            if existing_session_id and existing_thread_id:
                # ä½¿ç”¨ç°æœ‰sessionï¼Œé¿å…é‡å¤åˆ›å»º
                thread_id = existing_thread_id
                # é€šè¿‡thread_idè·å–sessionä¿¡æ¯
                session_data = await self.session_repo.get_session_by_thread_id(
                    existing_thread_id
                )
                if not session_data:
                    raise HTTPException(status_code=404, detail="æŒ‡å®šçš„sessionä¸å­˜åœ¨")
                url_param = session_data.url_param
                logger.info(
                    f"Using existing session: {existing_session_id}, thread_id: {thread_id}"
                )
            else:
                # åˆ›å»ºæ–°sessionï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                thread_id = str(uuid.uuid4())
                session_data, url_param = await self.session_repo.create_session(
                    thread_id=thread_id,
                    frontend_uuid=request.frontend_uuid,
                    visitor_id=request.visitor_id,
                    user_id=request.user_id,
                    initial_question=request.message,
                    research_config=research_config,
                    model_config=model_config,
                    output_config=output_config,
                )
                logger.info(
                    f"Created new session: {session_data.id}, thread_id: {thread_id}"
                )

            # åˆ›å»ºæ‰§è¡Œè®°å½•
            execution_record = await self.session_repo.create_execution_record(
                session_id=session_data.id,
                frontend_context_uuid=request.frontend_context_uuid,
                action_type=ActionType.CREATE,
                user_message=request.message,
            )
            execution_id = execution_record.execution_id

            # ğŸ”¥ ç§»é™¤é‡å¤çš„navigationäº‹ä»¶å‘é€
            # research_create_apiå·²ç»å‘é€äº†åŒ…å«session_idçš„navigationäº‹ä»¶
            # è¿™é‡Œä¸å†é‡å¤å‘é€ï¼Œé¿å…åŒé‡navigationäº‹ä»¶é—®é¢˜

            # å‡†å¤‡LangGraphåˆå§‹çŠ¶æ€
            initial_state = {
                "messages": [{"role": "user", "content": request.message}],
                "research_topic": request.message,
                "locale": output_config.get("language", "zh-CN"),
                "auto_accepted_plan": research_config.get(
                    "auto_accepted_plan", False
                ),  # ç”¨æˆ·å¯é…ç½®ï¼Œé»˜è®¤éœ€è¦ç¡®è®¤
                "enable_background_investigation": research_config.get(
                    "enable_background_investigation", True
                ),
                "plan_iterations": 0,
            }

            # è·å–LangGraphå®ä¾‹
            graph = await self._get_graph()

            # å¤„ç†LangGraphæµå¼æ‰§è¡Œ - ç›´æ¥æ‰§è¡Œï¼Œæ— éœ€é¢„åˆ›å»ºcheckpoint
            async for event in self._process_langgraph_stream(
                graph, initial_state, thread_id, execution_id, request, execution_type="create"
            ):
                yield event

        except Exception as e:
            logger.error(f"åˆ›å»ºç ”ç©¶æµå¤±è´¥: {e}")
            error_event = ErrorEvent(
                error_code="CREATE_STREAM_ERROR",
                error_message=str(e),
                error_details={"error_type": type(e).__name__},
                thread_id="",
                execution_id="",
                retry_after=30,
                suggestions=["æ£€æŸ¥è¯·æ±‚å‚æ•°", "ç¨åé‡è¯•"],
                timestamp=self._get_current_timestamp(),
            )

            yield {
                "event": SSEEventType.ERROR.value,
                "data": safe_json_dumps(asdict(error_event)),
            }

    async def continue_research_stream(
        self, request: ResearchStreamRequest
    ) -> AsyncGenerator[Dict[str, str], None]:
        """ç»§ç»­ç°æœ‰çš„ç ”ç©¶æµ - è·å–å†å²æ•°æ®å’Œå½“å‰çŠ¶æ€"""
        try:
            # ğŸ”¥ ä¼˜å…ˆä½¿ç”¨thread_idï¼Œå¦‚æœæ²¡æœ‰åˆ™é€šè¿‡url_paramè·å–
            thread_id = request.thread_id

            if not thread_id and request.url_param:
                # é€šè¿‡url_paramè·å–thread_id
                session = await self.session_repo.get_session_by_url_param(
                    request.url_param
                )
                if not session:
                    raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
                thread_id = session.thread_id

            if not thread_id:
                raise HTTPException(
                    status_code=400, detail="å¿…é¡»æä¾›thread_idæˆ–url_param"
                )

            logger.info(f"ğŸ” Attempting to connect to thread_id: {thread_id}")

            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰interrupt_feedbackï¼Œä¼˜å…ˆå¤„ç†
            interrupt_feedback = None
            if request.context and "interrupt_feedback" in request.context:
                interrupt_feedback = request.context["interrupt_feedback"]
                logger.info(f"ğŸ“‹ Processing interrupt feedback: {interrupt_feedback}")

                # æ„å»ºresumeæ¶ˆæ¯
                resume_msg = f"[{interrupt_feedback.upper()}]"
                if request.message and request.message.strip():
                    resume_msg += f" {request.message}"

                # ç”Ÿæˆæ‰§è¡ŒID - ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†UUIDæ ¼å¼
                execution_id = str(uuid.uuid4())

                # å‡†å¤‡Commandè¾“å…¥æ¥æ¢å¤æ‰§è¡Œ
                from langgraph.types import Command

                inputs = Command(resume=resume_msg)

                # è·å–LangGraphå®ä¾‹
                graph = await self._get_graph()

                # ä½¿ç”¨_process_langgraph_streamå¤„ç†æµå¼æ‰§è¡Œ
                async for event in self._process_langgraph_stream(
                    graph, inputs, thread_id, execution_id, request, execution_type="feedback"
                ):
                    yield event
                return

            # ğŸ”¥ åœºæ™¯åˆ¤æ–­
            if request.message and request.message.strip():
                # åœºæ™¯1ï¼šæœ‰æ–°æ¶ˆæ¯ï¼Œéœ€è¦ç»§ç»­æ‰§è¡Œ
                logger.info(f"ğŸ“ Continuing thread {thread_id} with new message")

                # ç”Ÿæˆæ‰§è¡ŒID - ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†UUIDæ ¼å¼
                execution_id = str(uuid.uuid4())

                # å‡†å¤‡è¾“å…¥çŠ¶æ€
                inputs = {"messages": [{"role": "user", "content": request.message}]}

                # è·å–LangGraphå®ä¾‹
                graph = await self._get_graph()

                # ä½¿ç”¨_process_langgraph_streamå¤„ç†æµå¼æ‰§è¡Œ
                async for event in self._process_langgraph_stream(
                    graph, inputs, thread_id, execution_id, request, execution_type="continue"
                ):
                    yield event
            else:
                # åœºæ™¯2ï¼šç›‘æ§æ¨¡å¼ - è·å–å†å²æ•°æ®å’Œå½“å‰çŠ¶æ€
                logger.info(f"ğŸ”„ Entering monitoring mode for thread {thread_id}")

                # ç”Ÿæˆç›‘å¬ä¼šè¯ID - ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†UUIDæ ¼å¼
                monitoring_id = str(uuid.uuid4())

                # å‘é€metadataäº‹ä»¶è¡¨ç¤ºå¼€å§‹è¿æ¥
                metadata_event = MetadataEvent(
                    execution_id=monitoring_id,
                    thread_id=thread_id,
                    session_id=None,
                    frontend_uuid=request.frontend_uuid,
                    frontend_context_uuid=request.frontend_context_uuid,
                    visitor_id=request.visitor_id,
                    user_id=request.user_id,
                    config_used=request.config,
                    model_info={
                        "model_name": "monitoring",
                        "provider": "database",
                        "version": "2.0",
                    },
                    estimated_duration=0,
                    start_time=self._get_current_timestamp(),
                    execution_type="monitor",
                    timestamp=self._get_current_timestamp(),
                )

                yield {
                    "event": SSEEventType.METADATA.value,
                    "data": safe_json_dumps(asdict(metadata_event)),
                }

                # ğŸ”¥ è·å–å†å²æ•°æ®
                session = await self.session_repo.get_session_by_thread_id(thread_id)
                if not session:
                    logger.error(f"âŒ Session not found for thread_id: {thread_id}")

                    error_event = ErrorEvent(
                        error_code="SESSION_NOT_FOUND",
                        error_message=f"æœªæ‰¾åˆ°thread_idå¯¹åº”çš„ä¼šè¯: {thread_id}",
                        error_details={"thread_id": thread_id},
                        thread_id=thread_id,
                        execution_id=monitoring_id,
                        retry_after=None,
                        suggestions=["æ£€æŸ¥thread_idæ˜¯å¦æ­£ç¡®", "ç¡®è®¤ä¼šè¯æ˜¯å¦å­˜åœ¨"],
                        timestamp=self._get_current_timestamp(),
                    )

                    yield {
                        "event": SSEEventType.ERROR.value,
                        "data": safe_json_dumps(asdict(error_event)),
                    }
                    return

                logger.info(
                    f"Using existing session: {session.id}, thread_id: {thread_id}"
                )

                # ğŸ”¥ è·å–LangGraphçŠ¶æ€æ¥åˆ¤æ–­ä»»åŠ¡æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                try:
                    graph = await self._get_graph()
                    config = {"configurable": {"thread_id": thread_id}}

                    # è·å–å½“å‰çŠ¶æ€
                    current_state = await graph.aget_state(config)
                    is_running = (
                        current_state
                        and current_state.next
                        and len(current_state.next) > 0
                    )

                    logger.info(
                        f"ğŸ” Task status - Running: {is_running}, Next nodes: {current_state.next if current_state else 'None'}"
                    )

                except Exception as state_error:
                    logger.warning(f"âš ï¸ Could not get LangGraph state: {state_error}")
                    is_running = False

                # è·å–å†å²æ¶ˆæ¯
                messages = await self.session_repo.get_messages_by_session_id(
                    session.id
                )

                # å‘é€å†å²æ¶ˆæ¯ä½œä¸ºäº‹ä»¶æµ
                for idx, msg in enumerate(messages):
                    chunk_event = MessageChunkEvent(
                        chunk_id=f"history_{idx}",
                        content=msg.content,
                        chunk_type="history",
                        agent_name=msg.source_agent or "assistant",
                        sequence=idx,
                        is_final=True,
                        metadata={
                            "role": msg.role,
                            "content_type": msg.content_type,
                            "created_at": (
                                msg.created_at.isoformat()
                                if hasattr(msg.created_at, "isoformat")
                                else str(msg.created_at)
                            ),
                            "is_historical": True,
                            "task_running": is_running,
                        },
                        thread_id=thread_id,
                        execution_id=monitoring_id,
                        timestamp=self._get_current_timestamp(),
                    )

                    yield {
                        "event": SSEEventType.MESSAGE_CHUNK.value,
                        "data": safe_json_dumps(asdict(chunk_event)),
                    }

                # ğŸ”¥ å¦‚æœä»»åŠ¡è¿˜åœ¨è¿è¡Œï¼Œé€šçŸ¥å‰ç«¯å¯èƒ½æœ‰æ–°çš„æ›´æ–°
                if is_running:
                    # å‘é€è¿›åº¦äº‹ä»¶è¡¨ç¤ºä»»åŠ¡ä»åœ¨è¿›è¡Œ
                    progress_event = ProgressEvent(
                        current_node=(
                            current_state.next[0] if current_state.next else "unknown"
                        ),
                        completed_nodes=[],
                        remaining_nodes=(
                            current_state.next if current_state.next else []
                        ),
                        current_step_description="ä»»åŠ¡æ­£åœ¨åå°è¿è¡Œä¸­...",
                        thread_id=thread_id,
                        execution_id=monitoring_id,
                        timestamp=self._get_current_timestamp(),
                    )

                    yield {
                        "event": SSEEventType.PROGRESS.value,
                        "data": safe_json_dumps(asdict(progress_event)),
                    }

                # å‘é€å®Œæˆäº‹ä»¶
                complete_event = CompleteEvent(
                    execution_id=monitoring_id,
                    thread_id=thread_id,
                    total_duration_ms=0,
                    tokens_consumed={"input": 0, "output": 0},
                    total_cost=0.0,
                    artifacts_generated=[],
                    final_status=(
                        "monitoring_complete" if not is_running else "task_running"
                    ),
                    completion_time=self._get_current_timestamp(),
                    summary={
                        "type": "monitoring",
                        "messages_count": len(messages),
                        "task_status": "running" if is_running else "completed",
                    },
                    timestamp=self._get_current_timestamp(),
                )

                yield {
                    "event": SSEEventType.COMPLETE.value,
                    "data": safe_json_dumps(asdict(complete_event)),
                }

        except Exception as e:
            logger.error(f"Continue research stream failed: {e}")
            error_event = ErrorEvent(
                error_code="CONTINUE_STREAM_ERROR",
                error_message=str(e),
                error_details={"error_type": type(e).__name__},
                thread_id=request.thread_id or "",
                execution_id="",
                retry_after=30,
                suggestions=["æ£€æŸ¥thread_id", "ç¡®è®¤ä¼šè¯å­˜åœ¨", "ç¨åé‡è¯•"],
                timestamp=self._get_current_timestamp(),
            )

            yield {
                "event": SSEEventType.ERROR.value,
                "data": safe_json_dumps(asdict(error_event)),
            }


# ä¾èµ–æ³¨å…¥
async def get_session_repository_dependency() -> SessionRepository:
    """è·å–SessionRepositoryä¾èµ–"""
    import os
    from dotenv import load_dotenv

    load_dotenv()
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        raise HTTPException(status_code=500, detail="æ•°æ®åº“é…ç½®é”™è¯¯")

    return get_session_repository(db_url)


# APIç«¯ç‚¹
@router.post("/stream")
async def research_stream(
    request: ResearchStreamRequest,
    session_repo: SessionRepository = Depends(get_session_repository_dependency),
):
    """ç»Ÿä¸€ç ”ç©¶æµå¼æ¥å£"""
    service = ResearchStreamService(session_repo)

    if request.action == ActionType.CREATE:
        event_generator = service.create_research_stream(request)
    else:
        event_generator = service.continue_research_stream(request)

    async def stream_events():
        async for event in event_generator:
            # SSEæ ¼å¼
            yield f"event: {event['event']}\n"
            yield f"data: {event['data']}\n\n"

    return StreamingResponse(
        stream_events(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


@router.get("/workspace/{url_param}")
async def get_workspace_data(
    url_param: str,
    session_repo: SessionRepository = Depends(get_session_repository_dependency),
):
    """è·å–å·¥ä½œåŒºçŠ¶æ€æ¥å£"""
    try:
        # é€šè¿‡url_paramè·å–ä¼šè¯ä¿¡æ¯
        session = await session_repo.get_session_by_url_param(url_param)
        if not session:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

        # è·å–æ¶ˆæ¯å†å²
        messages_data = await session_repo.get_messages_by_session_id(session.id)

        # è·å–æ‰§è¡Œè®°å½•
        executions_data = await session_repo.get_execution_records_by_session_id(
            session.id
        )

        # ğŸ”¥ è·å–artifacts
        artifacts_data = []
        try:
            async with await session_repo.get_connection() as conn:
                cursor = conn.cursor()
                await cursor.execute(
                    """
                    SELECT artifact_id, type, title, description, content, 
                           content_format, source_agent, created_at
                    FROM artifact_storage 
                    WHERE session_id = %s 
                    ORDER BY created_at DESC
                """,
                    (session.id,),
                )
                artifacts_rows = await cursor.fetchall()

                artifacts_data = [
                    {
                        "id": row["artifact_id"],
                        "type": row["type"],
                        "title": row["title"],
                        "description": row["description"],
                        "content": row["content"],
                        "format": row["content_format"],
                        "source_agent": row["source_agent"],
                        "created_at": (
                            row["created_at"].isoformat() if row["created_at"] else None
                        ),
                    }
                    for row in artifacts_rows
                ]
        except Exception as e:
            logger.error(f"è·å–artifactså¤±è´¥: {e}")

        # è·å–é…ç½®
        config = await session_repo.get_session_config(session.id)

        return {
            "thread_id": session.thread_id,
            "url_param": session.url_param,
            "status": (
                session.status
                if isinstance(session.status, str)
                else session.status.value
            ),
            "session_metadata": {
                "created_at": session.created_at.isoformat(),
                "last_updated": session.updated_at.isoformat(),
                "total_interactions": len(executions_data),
                "execution_history": [
                    {
                        "execution_id": exec.get("execution_id"),
                        "action_type": exec.get("action_type"),
                        "start_time": (
                            exec.get("created_at").isoformat()
                            if exec.get("created_at")
                            else None
                        ),
                        "status": exec.get("status"),
                    }
                    for exec in executions_data
                ],
            },
            "messages": [
                {
                    "id": msg.get("message_id"),
                    "content": msg.get("content"),
                    "role": msg.get("role"),
                    "timestamp": (
                        msg.get("timestamp").isoformat()
                        if msg.get("timestamp")
                        else None
                    ),
                    "metadata": {
                        "frontend_context_uuid": msg.get("frontend_context_uuid"),
                        "source_agent": msg.get("source_agent"),
                    },
                }
                for msg in messages_data
            ],
            "artifacts": artifacts_data,
            "config": {
                "current_config": (
                    {
                        "research_config": config.research_config or {},
                        "model_config": config.model_config or {},
                        "output_config": config.output_config or {},
                        "user_preferences": config.user_preferences or {},
                    }
                    if config
                    else {}
                ),
                "config_history": [],
            },
            "execution_stats": {
                "total_tokens_used": sum(
                    (exec.get("input_tokens", 0) or 0)
                    + (exec.get("output_tokens", 0) or 0)
                    for exec in executions_data
                ),
                "total_cost": sum(
                    exec.get("total_cost", 0) or 0 for exec in executions_data
                ),
                "average_response_time": (
                    sum(exec.get("duration_ms", 0) or 0 for exec in executions_data)
                    / len(executions_data)
                    if executions_data
                    else 0
                ),
                "model_usage_breakdown": {},
            },
            "permissions": {
                "can_modify": True,
                "can_share": True,
                "can_export": True,
                "can_delete": True,
            },
        }

    except Exception as e:
        logger.error(f"è·å–å·¥ä½œåŒºæ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
